<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains</title>
	<meta property="og:image" content="./resources/stablecxr_teaser.jpg"/>
	<meta property="og:title" content="Adapting Pretrained Vision-Language Foundational Models</br>to Medical Imaging Domains." />
	<meta property="og:description" content="Using Stable Diffusion to generate realistic-looking medical images." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Adapting Pretrained Vision-Language Foundational Models</br>to Medical Imaging Domains</span>
		</br></br>
		<table align=center width=600px>
			<table align=center width=450px>
				<tr style="font-size: 18px">
					<td align=center width=120px >
						<center>
							<span><a href="https://www.linkedin.com/in/pierre-chambon/">Pierre Chambon<sup>(*)</sup></a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span><a href="https://www.linkedin.com/in/bluethgen/">Christian Bluethgen<sup>(*)</sup></a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span><a href="https://profiles.stanford.edu/curtis-langlotz">Curt Langlotz</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span><a href="https://profiles.stanford.edu/akshay-chaudhari">Akshay Chaudhari</a></span>
						</center>
					</td>
				</tr>
			</table>
		<span style="text-align: center;">Center for Artificial Intelligence in Medicine and Imaging (AIMI), Stanford University</span>
		</br></br></br>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:700px; border: 0px;" src="./resources/stablecxr_teaser.jpg"/>
					</center>
				</td>
			</tr>
		</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2112.10752'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='./resources/bibtex.txt'>[BibTeX]</a></span><br>
						</center>
					</td>
					<!-- <td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='#' style="color:gray">[GitHub]</a></span><br>
						</center>
					</td> -->
				</tr>
			</table>
		</table>
	</center>

	<center>

		<table align=center width=850px>
			<tr>
				<td>
					
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td style='text-align: justify;'>
				<p >Multi-modal foundation models are typically trained on millions of pairs of naturalimages and text captions, frequently obtained through web-crawling approaches. Although such models depict excellent generative capabilities, they do not typically generalize well to specific domains such as medical images that have fundamentally shifted distributions compared to natural images. Building generative models for medical images that faithfully depict clinical context may help alleviate the paucity of healthcare datasets.
				<p>Thus, in this study, we seek to research and expand the representational capabilities of large pretrained foundation models to medical concepts, specifically for leveraging the Stable Diffusion model to generate domain-specific images found in medical imaging. We explore the sub-components ofthe Stable Diffusion pipeline (the variational autoencoder, the U-Net and the text-encoder) to fine-tune the model to generate medical images. We benchmark the efficacy of these efforts using quantitative image quality metrics and qualitative radiologist-driven evaluations that accurately represent the clinical content of conditional text prompts.</p>
				<p>Our best-performing model improves upon the Stable Diffusion baseline and can be conditioned to insert a realistic-looking abnormality on a synthetic radiology image, while maintaining a 95% accuracy on a classifier trained to detect the abnormality.</p>
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<!-- <center><h1>Code</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table> -->

	<br>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/stablecxr_gallery.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Add Description
				</td>
			</tr>
		</center>
	</table>
	<!-- <table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='#'>[GitHub]</a>
			</center>
		</span>
	</table> -->
	<br>
	<hr>
	<table align=center width=550px>
		<center><h1>Paper</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/stablecxr_arxiv_mini.png"/></a></td>
			<td><span style="font-size:14pt">P. Chambon, C. Bluethgen,<br>CP. Langlotz, A. Chaudhari<br>
				<b>Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains.</b><br>
				In Proceedings, 2022.<br>
				(hosted on <a href="#"><strike>ArXiv</strike></a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					<p style="text-align: justify;">Research reported in this publication was made possible in part by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) of the National Institutes of Health, which funded PC under contracts 75N92020C00008 and 75N92020C00021. CB received support from the Swiss Society of Radiology and the Kurt and Senta Herrmann-Foundation, unrelated to this work. We acknowledge the support of this work by the Wu Tsai Human Performance Alliance at Stanford University and the Joe and Clara Tsai Foundation.</p>
				</left>
			</td>
		</tr>
	</table>

<br />
<center><span style="font-size: 10px; text-align: center;">This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.</span></center>
<br />
</body>
</html>

